{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Dense Vector Representations\n",
    "Storing dense vectors in PyTorch: The `Embedding` and `EmbeddingBag` classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12beb7210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(4321)\n",
    "torch.manual_seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use the class `Embedding(num_embeddings, embedding_dim, ...)` and we create 5000 dense vectors (embeddings) in a vector space of dimension 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5000, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(5000, 64)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4716, -0.3436, -1.1742,  0.1221,  1.3231],\n",
       "        [-1.1889,  0.8678,  2.0916, -1.2002, -0.5946],\n",
       "        [ 2.1430, -0.3934,  0.0314, -0.6845, -3.5251],\n",
       "        [ 1.9011, -0.0659, -0.9426,  0.4688, -0.5444],\n",
       "        [-0.6471, -1.9928,  1.3672, -1.6397, -0.1779]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer acts as a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9011, -0.0659, -0.9426,  0.4688, -0.5444],\n",
       "        [ 2.1430, -0.3934,  0.0314, -0.6845, -3.5251],\n",
       "        [-1.1889,  0.8678,  2.0916, -1.2002, -0.5946]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx = torch.LongTensor([3, 2, 1])\n",
    "embedding(word_idx)[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input has a variable length, we have to align vectors up to a maximal length. We need then a padding symbol for the sequences less than this maximal length. We tell Torch by assigning the padding symbol an index usually 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(5000, 64, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5000, 64, padding_idx=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.7611,  0.9961,  0.0208, -1.5157, -0.3545],\n",
       "        [ 0.0871, -0.0454,  0.9313, -0.8555,  0.3771],\n",
       "        [ 0.6364,  1.1133,  0.8546,  0.7088, -1.0786],\n",
       "        [ 0.0289,  0.8395,  0.8943,  0.0564,  0.5867]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0871, -0.0454,  0.9313, -0.8555,  0.3771],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2631,  0.3616, -0.0205, -0.2178,  0.3249]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx = torch.LongTensor([0, 2, 0, 5])\n",
    "embedding(word_idx)[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bags\n",
    "Embedding bags deal with embedding sequences of variable length when the embeddings are summed. In CLD3, we have a weighted sum of a variable number of embeddings. See https://github.com/google/cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_bag = nn.EmbeddingBag(8, 5, mode='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2474, -0.7485, -0.4725,  0.9527,  1.4553],\n",
       "        [ 1.4757,  0.3881,  0.2289, -0.3961,  0.0492],\n",
       "        [-0.2349,  0.9298, -1.0225,  0.0796,  0.5364],\n",
       "        [-0.6228, -0.9396, -0.3125, -1.1847,  0.7243],\n",
       "        [-0.3061,  0.8004, -0.6700,  1.3515, -0.9943],\n",
       "        [ 0.2663,  0.3603,  1.4713, -1.1935,  1.5441],\n",
       "        [-0.9372, -0.3915, -0.0108, -0.2004,  0.3531],\n",
       "        [ 1.0411, -0.2576, -0.5129, -1.3512,  0.7840]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an `EmbeddingBag` object needs the bags of indices it will sum as its first parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2408,  1.3179, -0.7936, -0.3164,  0.5856],\n",
       "        [-0.9289, -0.1393, -0.9825,  0.1669, -0.2700]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2408,  1.3179, -0.7936, -0.3164,  0.5856]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1]])) + embedding_bag(torch.tensor([[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2408,  1.3179, -0.7936, -0.3164,  0.5856], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight[1] + embedding_bag.weight[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we may have a 1-D input and the the bag indices as second parameter: `offsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2408,  1.3179, -0.7936, -0.3164,  0.5856],\n",
       "        [-0.9289, -0.1393, -0.9825,  0.1669, -0.2700]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]), offsets=torch.tensor([0, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute a weighted sum using the `per_sample_weights` parameter. The shape must be the same as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6204,  0.6590, -0.3968, -0.1582,  0.2928],\n",
       "        [-0.3694,  0.4524, -0.5985,  0.8443, -0.6506]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]), per_sample_weights=torch.tensor(\n",
    "    [[0.5, 0.5], [0.2, 0.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6204,  0.6590, -0.3968, -0.1582,  0.2928], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * embedding_bag.weight[1] + 0.5 * embedding_bag.weight[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3694,  0.4524, -0.5985,  0.8443, -0.6506], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 * embedding_bag.weight[3] + 0.8 * embedding_bag.weight[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6204,  0.6590, -0.3968, -0.1582,  0.2928],\n",
       "        [-0.3694,  0.4524, -0.5985,  0.8443, -0.6506]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]),\n",
    "              offsets=torch.tensor([0, 2]),\n",
    "              per_sample_weights=torch.tensor([0.5, 0.5, 0.2, 0.8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
