{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Words, Parts of Speech, and Morphology\n",
    "## CoNLL file readers and writers.\n",
    "Use a class modeled as a vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'https://raw.githubusercontent.com/UniversalDependencies/'\n",
    "corpus_suffixes = ['train.conllu', 'dev.conllu', 'test.conllu']\n",
    "if CORPUS == 'English':\n",
    "    url = prefix + 'UD_English-EWT/master/en_ewt-ud-'\n",
    "elif CORPUS == 'French':\n",
    "    url = prefix + 'UD_French-GSD/master/fr_gsd-ud-'\n",
    "elif CORPUS == 'Spanish':\n",
    "    url = prefix + 'UD_Spanish-AnCora/master/es_ancora-ud-'\n",
    "else:\n",
    "    pass\n",
    "[train_file, val_file, test_file] = map(lambda x: url + x, corpus_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu',\n",
       " 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu',\n",
       " 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_file, val_file, test_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = urlopen(train_file).read().decode('utf-8').strip()\n",
    "val_sentences = urlopen(val_file).read().decode('utf-8').strip()\n",
    "test_sentences = urlopen(test_file).read().decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Token(dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names,\n",
    "                 sent_sep='\\n\\n',\n",
    "                 col_sep='\\t+'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS',\n",
    "             'HEAD', 'DEPREL', 'HEAD', 'DEPS', 'MISC']\n",
    "\n",
    "# column_names = list(map(str.lower, column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(col_names)\n",
    "train_dict = conll_dict.transform(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the form with the lemma and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '1',\n",
       "  'FORM': 'Or',\n",
       "  'LEMMA': 'or',\n",
       "  'UPOS': 'CCONJ',\n",
       "  'XPOS': 'CC',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '4:cc',\n",
       "  'DEPREL': 'cc',\n",
       "  'DEPS': '_'},\n",
       " {'ID': '2',\n",
       "  'FORM': 'you',\n",
       "  'LEMMA': 'you',\n",
       "  'UPOS': 'PRON',\n",
       "  'XPOS': 'PRP',\n",
       "  'FEATS': 'Case=Nom|Person=2|PronType=Prs',\n",
       "  'HEAD': '4:nsubj',\n",
       "  'DEPREL': 'nsubj',\n",
       "  'DEPS': '_'},\n",
       " {'ID': '3',\n",
       "  'FORM': 'can',\n",
       "  'LEMMA': 'can',\n",
       "  'UPOS': 'AUX',\n",
       "  'XPOS': 'MD',\n",
       "  'FEATS': 'VerbForm=Fin',\n",
       "  'HEAD': '4:aux',\n",
       "  'DEPREL': 'aux',\n",
       "  'DEPS': '_'},\n",
       " {'ID': '4',\n",
       "  'FORM': 'visit',\n",
       "  'LEMMA': 'visit',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB',\n",
       "  'FEATS': 'VerbForm=Inf',\n",
       "  'HEAD': '0:root',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '_'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[8131][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORM: Or\n",
      "LEMMA: or + _\n",
      "FORM: you\n",
      "LEMMA: you + Case=Nom|Person=2|PronType=Prs\n",
      "FORM: can\n",
      "LEMMA: can + VerbForm=Fin\n",
      "FORM: visit\n",
      "LEMMA: visit + VerbForm=Inf\n",
      "FORM: temples\n",
      "LEMMA: temple + Number=Plur\n",
      "FORM: or\n",
      "LEMMA: or + _\n",
      "FORM: shrines\n",
      "LEMMA: shrine + Number=Plur\n",
      "FORM: in\n",
      "LEMMA: in + _\n",
      "FORM: Okinawa\n",
      "LEMMA: Okinawa + Number=Sing\n",
      "FORM: .\n",
      "LEMMA: . + _\n"
     ]
    }
   ],
   "source": [
    "for word in train_dict[8131]:\n",
    "    print('FORM: {}'.format(word['FORM']))\n",
    "    print('LEMMA: {} + {}'.format(word['LEMMA'], word['FEATS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'w') as f:\n",
    "    json.dump(train_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['ID', 'FORM', 'LEMMA', 'CPOS', 'POS', 'FEATS'])\n",
      "The form: La\n",
      "Is key form in token? False\n"
     ]
    }
   ],
   "source": [
    "# column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "tok = Token({'ID': '1', 'FORM': 'La', 'LEMMA': 'el',\n",
    "            'CPOS': 'd', 'POS': 'da', 'FEATS': 'num=s|gen=f'})\n",
    "print('Keys:', tok.keys())\n",
    "print('The form:', tok['FORM'])\n",
    "print('Is key form in token?', 'form' in tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: {'feats', 'cpos', 'pos', 'form', 'lemma', 'id'}\n",
      "{'lemma', 'feats', 'cpos', 'pos', 'form', 'id'}\n"
     ]
    }
   ],
   "source": [
    "tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el',\n",
    "            'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el',\n",
    "             'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "tok_set = set(tok_dict)\n",
    "print('Keys:', tok_set)\n",
    "tok_set = tok_set.union(tok_dict2)\n",
    "print(tok_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: ['num=s|gen=f', 'La', 'da', 'd', '1', 'el']\n",
      "['num=s|gen=f', 'La', 'da', 'd', '1', 'el']\n",
      "Values: ['num=s|gen=f', 'La', 'da', 'd', '1', 'el']\n",
      "Token values: dict_values(['1', 'La', 'el', 'd', 'da', 'num=s|gen=f'])\n",
      "Values: ['num=s|gen=f', 'La', 'da', 'd', '1', 'el']\n"
     ]
    }
   ],
   "source": [
    "word_set = set(tok_dict.values())\n",
    "print('Values:', list(word_set))\n",
    "\n",
    "word_set = set(tok.values())\n",
    "print(list(word_set))\n",
    "\n",
    "word_set = set()\n",
    "word_set.update(tok.values())\n",
    "print('Values:', list(word_set))\n",
    "\n",
    "word_set = set()\n",
    "print(\"Token values:\", tok.values())\n",
    "word_set = word_set.union(set(tok.values()))\n",
    "print('Values:', list(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves a file in the CoNLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conll(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += '\\t'.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_conll('out', train_dict, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
