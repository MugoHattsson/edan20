{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Subword Segmentation\n",
    "## Subword Tokenizers: BPE\n",
    "The implementation of BPE from the Gage and Sennrich papers.\n",
    "\n",
    "A valuable complete implementation of the algorithm: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the files and we store the corpus in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'HOMER'  # 'DICKENS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CORPUS == 'DICKENS':\n",
    "    folder = PATH + 'dickens/'\n",
    "elif CORPUS == 'HOMER':\n",
    "    folder = PATH + 'classics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iliad.txt', 'odyssey.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CORPUS == 'DICKENS':\n",
    "    files = get_files(folder, 'txt')\n",
    "elif CORPUS == 'HOMER':\n",
    "    files = ['iliad.txt', 'odyssey.txt']\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/classics/iliad.txt', '../datasets/classics/odyssey.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [folder + file for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for file in files:\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        text += ' ' + f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BOOK I\\n\\nSing, O goddess, the anger of Achilles son of Peleus, that brought\\ncountless ills upon the '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pretokenize the text using the spaces as delimiters. First try the `'simple'` tokenization, where the word start is not visible, then the `'spaces'` where leading spaces will show as `'Ġ'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenization = 'simple'\n",
    "if pretokenization == 'simple':\n",
    "    pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "elif pretokenization == 'spaces':\n",
    "    pattern = r' ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+'\n",
    "elif pretokenization == 'karpathy':\n",
    "    pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(match.group(), (match.start(), match.end()))\n",
    "         for match in re.finditer(pattern, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOOK', (1, 5)),\n",
       " ('I', (6, 7)),\n",
       " ('Sing', (9, 13)),\n",
       " (',', (13, 14)),\n",
       " ('O', (15, 16)),\n",
       " ('goddess', (17, 24)),\n",
       " (',', (24, 25)),\n",
       " ('the', (26, 29))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_1(pattern, text):\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_2(pattern, text):\n",
    "    words = re.findall(pattern, text)\n",
    "    return [''.join(('Ġ', word[1:]))\n",
    "            if word[0] == ' ' else word\n",
    "            for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretokenization == 'simple':\n",
    "    pretokenize = pretokenize_1\n",
    "else:\n",
    "    pretokenize = pretokenize_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pretokenize(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19410), ('the', 15258), ('and', 11467), ('of', 8640), ('.', 6839)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnts['her']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\p{L}+|\\p{N}+|\\p{P}|[^\\s\\p{L}\\p{N}\\p{P}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wait', '.', '.', '.', 'here', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pat, 'Wait... here!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class with Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "words = bpe.pretokenize(text)\n",
    "word_cnts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19410), ('the', 15258), ('and', 11467), ('of', 8640), ('.', 6839)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnts.most_common(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Vocabulary\n",
    "We create a second dictionary to count the subword tokens. At each iteration, the keys will store the subtokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                 for char in self.words_bpe[word]['swords']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe._bpe_init(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 1145, 'swords': ['h', 'e', 'r']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.words_bpe['her']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the bigrams from the `swords` lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe._bpe_init(text)\n",
    "bpe._count_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h', 'e')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bpe.pair_cnts, key=bpe.pair_cnts.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging a Pair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge a pair in a sequence of subwords. The structure of the pair is a list as in: `['h', 'e']`. `swords` is also a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq\n",
    "\n",
    "    def _merge_pair(self, pair, swords):\n",
    "        pair_str = ''.join(pair)\n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(swords) - 1:\n",
    "            if pair == swords[i:i + 2]:\n",
    "                temp += [pair_str]\n",
    "                i += 2\n",
    "            else:\n",
    "                temp += [swords[i]]\n",
    "                i += 1\n",
    "        if i == len(swords) - 1:\n",
    "            temp += [swords[i]]\n",
    "        swords = temp\n",
    "        return swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe._bpe_init(text)\n",
    "bpe._count_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'he', 'y']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe._merge_pair(['h', 'e'], ['t', 'h', 'e', 'y'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, merge_cnt=200):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "        self.merge_cnt = merge_cnt\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq\n",
    "\n",
    "    def _merge_pair(self, pair, swords):\n",
    "        pair_str = ''.join(pair)\n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(swords) - 1:\n",
    "            if pair == swords[i:i + 2]:\n",
    "                temp += [pair_str]\n",
    "                i += 2\n",
    "            else:\n",
    "                temp += [swords[i]]\n",
    "                i += 1\n",
    "        if i == len(swords) - 1:\n",
    "            temp += [swords[i]]\n",
    "        swords = temp\n",
    "        return swords\n",
    "\n",
    "    def fit(self, text):\n",
    "        self._bpe_init(text)\n",
    "\n",
    "        self.merge_ops = []\n",
    "        for _ in range(self.merge_cnt):\n",
    "            self._count_bigrams()\n",
    "            self.best_pair = max(self.pair_cnts,\n",
    "                                 key=self.pair_cnts.get)\n",
    "            merge_op = list(self.best_pair)\n",
    "            self.merge_ops.append(merge_op)\n",
    "            for word_dict in self.words_bpe.values():\n",
    "                word_dict['swords'] = self._merge_pair(\n",
    "                    merge_op,\n",
    "                    word_dict['swords'])\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        swords = list(map(lambda x: ''.join(x), self.merge_ops))\n",
    "        self.vocab += swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h', 'e'], ['t', 'he'], ['a', 'n'], ['i', 'n'], ['o', 'u']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.merge_ops[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v', 'r', 'W', 'T', 'Q', 'R', '[', '!', 'c', 'A']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding a Word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the rules in the same order.\n",
    "\n",
    "We can define a vocabulary consisting of the characters in the training corpus and the subwords. The characters outside this set will be mapped to `'UNK'`. Otherwise the initial vocabulary consists of all the Unicode characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, merge_cnt=200):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "        self.merge_cnt = merge_cnt\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq\n",
    "\n",
    "    def _merge_pair(self, pair, swords):\n",
    "        pair_str = ''.join(pair)\n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(swords) - 1:\n",
    "            if pair == swords[i:i + 2]:\n",
    "                temp += [pair_str]\n",
    "                i += 2\n",
    "            else:\n",
    "                temp += [swords[i]]\n",
    "                i += 1\n",
    "        if i == len(swords) - 1:\n",
    "            temp += [swords[i]]\n",
    "        swords = temp\n",
    "        return swords\n",
    "\n",
    "    def fit(self, text):\n",
    "        self._bpe_init(text)\n",
    "\n",
    "        self.merge_ops = []\n",
    "        for _ in range(self.merge_cnt):\n",
    "            self._count_bigrams()\n",
    "            self.best_pair = max(self.pair_cnts,\n",
    "                                 key=self.pair_cnts.get)\n",
    "            merge_op = list(self.best_pair)\n",
    "            self.merge_ops.append(merge_op)\n",
    "            for word_dict in self.words_bpe.values():\n",
    "                word_dict['swords'] = self._merge_pair(\n",
    "                    merge_op,\n",
    "                    word_dict['swords'])\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        swords = list(map(lambda x: ''.join(x), self.merge_ops))\n",
    "        self.vocab += swords\n",
    "\n",
    "    def encode(self, word):\n",
    "        swords = list(word)\n",
    "        for op in self.merge_ops:\n",
    "            swords = self._merge_pair(op, swords)\n",
    "        return swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there', 'fore']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode('therefore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Whole Text\n",
    "We can now write the complete subword tokenization function. We use a cache to speed up the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, merge_cnt=200):\n",
    "        self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "        self.merge_cnt = merge_cnt\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        return re.findall(self.pattern, text)\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq\n",
    "\n",
    "    def _merge_pair(self, pair, swords):\n",
    "        pair_str = ''.join(pair)\n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(swords) - 1:\n",
    "            if pair == swords[i:i + 2]:\n",
    "                temp += [pair_str]\n",
    "                i += 2\n",
    "            else:\n",
    "                temp += [swords[i]]\n",
    "                i += 1\n",
    "        if i == len(swords) - 1:\n",
    "            temp += [swords[i]]\n",
    "        swords = temp\n",
    "        return swords\n",
    "\n",
    "    def fit(self, text):\n",
    "        self._bpe_init(text)\n",
    "\n",
    "        self.merge_ops = []\n",
    "        for _ in range(self.merge_cnt):\n",
    "            self._count_bigrams()\n",
    "            self.best_pair = max(self.pair_cnts,\n",
    "                                 key=self.pair_cnts.get)\n",
    "            merge_op = list(self.best_pair)\n",
    "            self.merge_ops.append(merge_op)\n",
    "            for word_dict in self.words_bpe.values():\n",
    "                word_dict['swords'] = self._merge_pair(\n",
    "                    merge_op,\n",
    "                    word_dict['swords'])\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        swords = list(map(lambda x: ''.join(x), self.merge_ops))\n",
    "        self.vocab += swords\n",
    "\n",
    "    def encode(self, word):\n",
    "        swords = list(word)\n",
    "        for op in self.merge_ops:\n",
    "            swords = self._merge_pair(op, swords)\n",
    "        return swords\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokenized_text = []\n",
    "        cache = {}\n",
    "        words = self.pretokenize(text)\n",
    "        for word in words:\n",
    "            if word not in cache:\n",
    "                cache[word] = self.encode(word)\n",
    "            subwords = cache[word]\n",
    "            tokenized_text += subwords\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE()\n",
    "bpe.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecloges_str = \"\"\"Sit careless in the shade\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'it', 'c', 'are', 'le', 's', 's', 'in', 'the', 's', 'had', 'e']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.tokenize(ecloges_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leading Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r' ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, merge_cnt=200, leading_space=False):\n",
    "        self.merge_cnt = merge_cnt\n",
    "        self.leading_space = leading_space\n",
    "        if leading_space:\n",
    "            self.pattern = r' ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+'\n",
    "        else:\n",
    "            self.pattern = r'\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+'\n",
    "\n",
    "    def pretokenize(self, text):\n",
    "        words = re.findall(self.pattern, text)\n",
    "        if self.leading_space:\n",
    "            words = [''.join(('Ġ', word[1:]))\n",
    "                     if word[0] == ' ' else word\n",
    "                     for word in words]\n",
    "        return words\n",
    "\n",
    "    def _bpe_init(self, text):\n",
    "        words = self.pretokenize(text)\n",
    "        word_cnts = Counter(words)\n",
    "        self.words_bpe = {\n",
    "            word: {'freq': freq,\n",
    "                   'swords': list(word)}\n",
    "            for word, freq in word_cnts.items()}\n",
    "        self.vocab = list(\n",
    "            set([char for word in self.words_bpe\n",
    "                for char in self.words_bpe[word]['swords']]))\n",
    "\n",
    "    def _count_bigrams(self):\n",
    "        self.pair_cnts = Counter()\n",
    "        for word_dict in self.words_bpe.values():\n",
    "            swords = tuple(word_dict['swords'])\n",
    "            freq = word_dict['freq']\n",
    "            for i in range(len(swords) - 1):\n",
    "                self.pair_cnts[swords[i:i + 2]] += freq\n",
    "\n",
    "    def _merge_pair(self, pair, swords):\n",
    "        pair_str = ''.join(pair)\n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(swords) - 1:\n",
    "            if pair == swords[i:i + 2]:\n",
    "                temp += [pair_str]\n",
    "                i += 2\n",
    "            else:\n",
    "                temp += [swords[i]]\n",
    "                i += 1\n",
    "        if i == len(swords) - 1:\n",
    "            temp += [swords[i]]\n",
    "        swords = temp\n",
    "        return swords\n",
    "\n",
    "    def fit(self, text):\n",
    "        self._bpe_init(text)\n",
    "\n",
    "        self.merge_ops = []\n",
    "        for _ in range(self.merge_cnt):\n",
    "            self._count_bigrams()\n",
    "            self.best_pair = max(self.pair_cnts,\n",
    "                                 key=self.pair_cnts.get)\n",
    "            merge_op = list(self.best_pair)\n",
    "            self.merge_ops.append(merge_op)\n",
    "            for word_dict in self.words_bpe.values():\n",
    "                word_dict['swords'] = self._merge_pair(\n",
    "                    merge_op,\n",
    "                    word_dict['swords'])\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        swords = list(map(lambda x: ''.join(x), self.merge_ops))\n",
    "        self.vocab += swords\n",
    "\n",
    "    def encode(self, word):\n",
    "        swords = list(word)\n",
    "        for op in self.merge_ops:\n",
    "            swords = self._merge_pair(op, swords)\n",
    "        return swords\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokenized_text = []\n",
    "        cache = {}\n",
    "        words = self.pretokenize(text)\n",
    "        for word in words:\n",
    "            if word not in cache:\n",
    "                cache[word] = self.encode(word)\n",
    "            subwords = cache[word]\n",
    "            tokenized_text += subwords\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(leading_space=True)\n",
    "bpe.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'it', 'Ġc', 'a', 're', 'l', 'ess', 'Ġin', 'Ġthe', 'Ġsh', 'ad', 'e']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.tokenize('Sit careless in the shade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġsp',\n",
       " 'o',\n",
       " 'ke',\n",
       " 'ĠM',\n",
       " 'in',\n",
       " 'er',\n",
       " 'v',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġ',\n",
       " 'U',\n",
       " 'ly',\n",
       " 's',\n",
       " 'ses',\n",
       " 'Ġo',\n",
       " 'b',\n",
       " 'e',\n",
       " 'y',\n",
       " 'ed',\n",
       " 'Ġher',\n",
       " 'Ġg',\n",
       " 'l',\n",
       " 'ad',\n",
       " 'ly',\n",
       " '.',\n",
       " 'ĠT',\n",
       " 'hen',\n",
       " 'ĠM',\n",
       " 'in',\n",
       " 'er',\n",
       " 'v',\n",
       " 'a',\n",
       " 'Ġas',\n",
       " 's',\n",
       " 'u',\n",
       " 'm',\n",
       " 'ed',\n",
       " 'the',\n",
       " 'Ġfor',\n",
       " 'm',\n",
       " 'Ġand',\n",
       " 'Ġ',\n",
       " 'v',\n",
       " 'o',\n",
       " 'i',\n",
       " 'ce',\n",
       " 'Ġof',\n",
       " 'ĠM',\n",
       " 'ent',\n",
       " 'or',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġp',\n",
       " 're',\n",
       " 's',\n",
       " 'ent',\n",
       " 'ly',\n",
       " 'Ġm',\n",
       " 'ad',\n",
       " 'e',\n",
       " 'Ġa',\n",
       " 'Ġc',\n",
       " 'o',\n",
       " 'ven',\n",
       " 'an',\n",
       " 't',\n",
       " 'Ġof',\n",
       " 'Ġp',\n",
       " 'e',\n",
       " 'a',\n",
       " 'ce',\n",
       " 'b',\n",
       " 'et',\n",
       " 'w',\n",
       " 'e',\n",
       " 'en',\n",
       " 'Ġthe',\n",
       " 'Ġt',\n",
       " 'w',\n",
       " 'o',\n",
       " 'Ġc',\n",
       " 'on',\n",
       " 't',\n",
       " 'e',\n",
       " 'nd',\n",
       " 'ing',\n",
       " 'Ġp',\n",
       " 'ar',\n",
       " 't',\n",
       " 'i',\n",
       " 'es',\n",
       " '.',\n",
       " 'T',\n",
       " 'H',\n",
       " 'E',\n",
       " 'Ġ',\n",
       " 'E',\n",
       " 'N',\n",
       " 'D']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.tokenize(text)[-100:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_chars1 = range(0, 33)\n",
    "control_chars2 = range(127, 160)\n",
    "control_chars3 = range(173, 174)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_table = list(enumerate(list(control_chars1) +\n",
    "                   list(control_chars2) + list(control_chars3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shift_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(ord(' ') + 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtp2_special_bytes = {}\n",
    "for n, b in shift_table:\n",
    "    gtp2_special_bytes[b] = chr(n + 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ā',\n",
       " 1: 'ā',\n",
       " 2: 'Ă',\n",
       " 3: 'ă',\n",
       " 4: 'Ą',\n",
       " 5: 'ą',\n",
       " 6: 'Ć',\n",
       " 7: 'ć',\n",
       " 8: 'Ĉ',\n",
       " 9: 'ĉ',\n",
       " 10: 'Ċ',\n",
       " 11: 'ċ',\n",
       " 12: 'Č',\n",
       " 13: 'č',\n",
       " 14: 'Ď',\n",
       " 15: 'ď',\n",
       " 16: 'Đ',\n",
       " 17: 'đ',\n",
       " 18: 'Ē',\n",
       " 19: 'ē',\n",
       " 20: 'Ĕ',\n",
       " 21: 'ĕ',\n",
       " 22: 'Ė',\n",
       " 23: 'ė',\n",
       " 24: 'Ę',\n",
       " 25: 'ę',\n",
       " 26: 'Ě',\n",
       " 27: 'ě',\n",
       " 28: 'Ĝ',\n",
       " 29: 'ĝ',\n",
       " 30: 'Ğ',\n",
       " 31: 'ğ',\n",
       " 32: 'Ġ',\n",
       " 127: 'ġ',\n",
       " 128: 'Ģ',\n",
       " 129: 'ģ',\n",
       " 130: 'Ĥ',\n",
       " 131: 'ĥ',\n",
       " 132: 'Ħ',\n",
       " 133: 'ħ',\n",
       " 134: 'Ĩ',\n",
       " 135: 'ĩ',\n",
       " 136: 'Ī',\n",
       " 137: 'ī',\n",
       " 138: 'Ĭ',\n",
       " 139: 'ĭ',\n",
       " 140: 'Į',\n",
       " 141: 'į',\n",
       " 142: 'İ',\n",
       " 143: 'ı',\n",
       " 144: 'Ĳ',\n",
       " 145: 'ĳ',\n",
       " 146: 'Ĵ',\n",
       " 147: 'ĵ',\n",
       " 148: 'Ķ',\n",
       " 149: 'ķ',\n",
       " 150: 'ĸ',\n",
       " 151: 'Ĺ',\n",
       " 152: 'ĺ',\n",
       " 153: 'Ļ',\n",
       " 154: 'ļ',\n",
       " 155: 'Ľ',\n",
       " 156: 'ľ',\n",
       " 157: 'Ŀ',\n",
       " 158: 'ŀ',\n",
       " 159: 'Ł',\n",
       " 173: 'ł'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtp2_special_bytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
